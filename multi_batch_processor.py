#!/usr/bin/env python3
"""
Multi-Batch Processor - Wrapper para OCRBatchProcessor
======================================================
DEPRECATED: Este m√≥dulo se mantiene por compatibilidad con c√≥digo existente.
Usar batch_processor.OCRBatchProcessor directamente en c√≥digo nuevo.

Este archivo ahora act√∫a como wrapper simple sobre OCRBatchProcessor,
proporcionando la misma API que antes pero usando el procesador unificado.

Versi√≥n: 2.0.0 - Wrapper sobre batch_processor.py (Fase 4)
"""

import logging
from typing import List, Dict
from pathlib import Path

# Importar el procesador unificado y sus componentes
from batch_processor import (
    OCRBatchProcessor,
    FileEntry,
    MultiBatchSummary
)
from batch_optimizer import BatchOptimizer

logger = logging.getLogger(__name__)


# ==================== CLASE WRAPPER ====================

class MultiBatchProcessor(OCRBatchProcessor):
    """
    DEPRECATED: Usar OCRBatchProcessor de batch_processor.py

    Esta clase se mantiene por compatibilidad con c√≥digo existente que usa:
    - from multi_batch_processor import MultiBatchProcessor

    Hereda toda la funcionalidad de OCRBatchProcessor sin duplicar c√≥digo.
    """

    def __init__(self):
        """
        Inicializa MultiBatchProcessor (wrapper sobre OCRBatchProcessor)

        NOTA: OCRBatchProcessor requiere ocr_client en __init__, pero
        MultiBatchProcessor original no lo requer√≠a. Para compatibilidad,
        inicializamos sin cliente y se debe establecer antes de usar.
        """
        # No podemos llamar a super().__init__() sin ocr_client
        # Inicializamos solo el optimizer como en la versi√≥n original
        self.optimizer = BatchOptimizer()

        logger.warning(
            "MultiBatchProcessor est√° deprecado. "
            "Usar OCRBatchProcessor de batch_processor.py en c√≥digo nuevo."
        )

    def analyze_multiple_files(self, file_paths: List[str]) -> MultiBatchSummary:
        """
        Analiza m√∫ltiples archivos PDF y genera recomendaciones.

        Este m√©todo mantiene compatibilidad con la API original.

        Args:
            file_paths: Lista de rutas de archivos

        Returns:
            MultiBatchSummary con an√°lisis completo
        """
        logger.warning(
            "MultiBatchProcessor.analyze_multiple_files() est√° deprecado. "
            "Usar OCRBatchProcessor.analyze_multiple_files() en c√≥digo nuevo."
        )

        # Implementaci√≥n directa para evitar recursi√≥n infinita
        # Analizar cada archivo individualmente
        file_entries = []
        total_size = 0
        total_pages = 0
        files_requiring_split = 0

        for file_path in file_paths:
            try:
                path = Path(file_path)
                if not path.exists():
                    logger.warning(f"Archivo no encontrado: {file_path}")
                    continue

                # Obtener p√°ginas del PDF
                page_count = self._count_pdf_pages(path)
                if page_count is None:
                    continue

                # Analizar con BatchOptimizer
                analysis = self.optimizer.analyze_pdf(str(path), page_count)

                entry = FileEntry(
                    file_path=path,
                    display_name=path.name,
                    order_index=len(file_entries),
                    analysis=analysis,
                    recommendation=None  # Se genera despu√©s si se necesita
                )

                file_entries.append(entry)
                total_size += analysis.total_size_mb
                total_pages += analysis.total_pages

                if analysis.requires_splitting:
                    files_requiring_split += 1
                    entry.recommendation = self.optimizer.calculate_optimal_split(analysis)

            except Exception as e:
                logger.error(f"Error analizando {file_path}: {e}")

        # Calcular archivos estimados despu√©s de splits
        total_estimated_files = len(file_entries)
        for entry in file_entries:
            if entry.recommendation:
                total_estimated_files += entry.recommendation.num_files - 1

        # Determinar estrategia global
        if files_requiring_split == 0:
            global_strategy = "process_directly"
        elif files_requiring_split == len(file_entries):
            global_strategy = "split_all"
        else:
            global_strategy = "mixed"

        # Generar warnings si hay archivos que requieren split
        warnings = []
        if files_requiring_split > 0:
            warnings.append(f"{files_requiring_split} archivo(s) requieren divisi√≥n antes de procesamiento")

        # Crear resumen
        return MultiBatchSummary(
            files=file_entries,
            total_size_mb=total_size,
            total_pages=total_pages,
            total_estimated_files=total_estimated_files,
            global_strategy=global_strategy,
            processing_time_estimate=self._estimate_processing_time(total_pages) / 60.0,  # convertir a minutos
            warnings=warnings
        )

    def _count_pdf_pages(self, file_path: Path) -> int:
        """Cuenta p√°ginas de un PDF."""
        try:
            from PyPDF2 import PdfReader
            reader = PdfReader(file_path)
            return len(reader.pages)
        except Exception as e:
            logger.error(f"Error contando p√°ginas de {file_path}: {e}")
            return None

    def _estimate_processing_time(self, total_pages: int) -> float:
        """Estima tiempo de procesamiento en segundos."""
        # Aproximadamente 2 segundos por p√°gina
        return total_pages * 2.0

    def generate_processing_plan(self, summary: MultiBatchSummary) -> Dict:
        """
        Genera plan detallado de procesamiento.

        Args:
            summary: Resumen de an√°lisis m√∫ltiple

        Returns:
            Dict con plan de procesamiento
        """
        logger.warning(
            "MultiBatchProcessor.generate_processing_plan() est√° deprecado."
        )

        plan = {
            'files': [],
            'page_offset': 0,
            'total_operations': 0,
            'estimated_time': summary.processing_time_estimate
        }

        current_page_offset = 0

        for entry in summary.files:
            file_plan = {
                'original_file': str(entry.file_path),
                'display_name': entry.display_name,
                'page_offset': current_page_offset,
                'operations': []
            }

            if entry.recommendation and entry.analysis:
                if entry.recommendation.num_files == 1:
                    file_plan['operations'].append({
                        'type': 'direct_process',
                        'pages': entry.analysis.total_pages,
                        'estimated_mb': entry.analysis.total_size_mb
                    })
                else:
                    pages_per_file = entry.recommendation.pages_per_file
                    for i in range(entry.recommendation.num_files):
                        start_page = i * pages_per_file + 1
                        end_page = min((i + 1) * pages_per_file, entry.analysis.total_pages)

                        file_plan['operations'].append({
                            'type': 'split_and_process',
                            'part_number': i + 1,
                            'pages_range': (start_page, end_page),
                            'pages_count': end_page - start_page + 1,
                            'estimated_mb': entry.recommendation.estimated_mb_per_file
                        })

                current_page_offset += entry.analysis.total_pages
                plan['total_operations'] += len(file_plan['operations'])

            plan['files'].append(file_plan)

        return plan

    def format_summary_report(self, summary: MultiBatchSummary) -> str:
        """
        Genera reporte formateado del an√°lisis m√∫ltiple.

        Args:
            summary: Resumen de an√°lisis

        Returns:
            String con reporte formateado
        """
        logger.warning(
            "MultiBatchProcessor.format_summary_report() est√° deprecado."
        )

        lines = [
            "=" * 70,
            "AN√ÅLISIS DE PROCESAMIENTO M√öLTIPLE",
            "=" * 70,
            f"Archivos a procesar: {len(summary.files)}",
            f"Tama√±o total: {summary.total_size_mb:.1f} MB",
            f"P√°ginas totales: {summary.total_pages}",
            f"Densidad promedio: {summary.avg_density:.2f} MB/p√°gina",
            f"Archivos estimados tras divisi√≥n: {summary.total_estimated_files}",
            f"Tiempo estimado: {summary.processing_time_estimate:.1f} minutos",
            f"Estrategia global: {summary.global_strategy}",
            ""
        ]

        # Advertencias globales
        if summary.warnings:
            lines.extend([
                "‚ö†Ô∏è ADVERTENCIAS:",
                "-" * 30
            ])
            for warning in summary.warnings:
                lines.append(f"  ‚Ä¢ {warning}")
            lines.append("")

        # Detalles por archivo
        lines.extend([
            "AN√ÅLISIS POR ARCHIVO:",
            "-" * 70
        ])

        page_offset = 0
        for i, entry in enumerate(summary.files, 1):
            lines.append(f"\n{i}. {entry.display_name}")

            if entry.analysis:
                lines.extend([
                    f"   üìä Tama√±o: {entry.analysis.total_size_mb:.1f} MB",
                    f"   üìÑ P√°ginas: {entry.analysis.total_pages} (desde p√°gina {page_offset + 1})",
                    f"   üéØ Densidad: {entry.analysis.density_mb_per_page:.2f} MB/p√°gina"
                ])

                if entry.recommendation:
                    if entry.recommendation.num_files == 1:
                        lines.append("   ‚úÖ No requiere divisi√≥n")
                    else:
                        lines.extend([
                            f"   üìÇ Divisi√≥n: {entry.recommendation.num_files} archivos",
                            f"   üìë P√°ginas por archivo: {entry.recommendation.pages_per_file}",
                            f"   üíæ MB por archivo: {entry.recommendation.estimated_mb_per_file:.1f}",
                            f"   üèÜ Eficiencia: {entry.recommendation.efficiency_score:.0%}"
                        ])

                page_offset += entry.analysis.total_pages
            else:
                lines.append("   ‚ùå Error en an√°lisis")

        lines.extend([
            "",
            "=" * 70
        ])

        return "\n".join(lines)

    def get_file_processing_order(self, summary: MultiBatchSummary):
        """
        Obtiene orden de procesamiento con offsets de p√°gina.

        Args:
            summary: Resumen de an√°lisis

        Returns:
            Lista de tuplas (file_path, page_offset, total_pages)
        """
        logger.warning(
            "MultiBatchProcessor.get_file_processing_order() est√° deprecado."
        )

        processing_order = []
        page_offset = 0

        for entry in summary.files:
            if entry.analysis:
                processing_order.append((
                    str(entry.file_path),
                    page_offset,
                    entry.analysis.total_pages
                ))
                page_offset += entry.analysis.total_pages

        return processing_order


# ==================== FUNCIONES DE UTILIDAD ====================

def analyze_multiple_pdfs(file_paths: List[str]) -> MultiBatchSummary:
    """
    DEPRECATED: Usar OCRBatchProcessor.analyze_multiple_files()

    Funci√≥n principal para an√°lisis m√∫ltiple.

    Args:
        file_paths: Lista de rutas de archivos

    Returns:
        MultiBatchSummary con an√°lisis
    """
    logger.warning(
        "multi_batch_processor.analyze_multiple_pdfs() est√° deprecado. "
        "Usar OCRBatchProcessor.analyze_multiple_files() en c√≥digo nuevo."
    )

    processor = MultiBatchProcessor()
    return processor.analyze_multiple_files(file_paths)


def get_processing_plan(file_paths: List[str]) -> Dict:
    """
    DEPRECATED: Crear OCRBatchProcessor y usar sus m√©todos

    Obtiene plan completo de procesamiento.

    Args:
        file_paths: Lista de rutas de archivos

    Returns:
        Dict con plan de procesamiento
    """
    logger.warning(
        "multi_batch_processor.get_processing_plan() est√° deprecado."
    )

    processor = MultiBatchProcessor()
    summary = processor.analyze_multiple_files(file_paths)
    return processor.generate_processing_plan(summary)


# ==================== MENSAJE DE DEPRECACI√ìN ====================
# Mensaje cambiado a nivel DEBUG para no mostrar en uso normal
# Si necesitas ver estos mensajes, ejecuta con: logging.basicConfig(level=logging.DEBUG)

logger.debug(
    "‚ö†Ô∏è  multi_batch_processor.py est√° deprecado. "
    "Toda la funcionalidad se ha movido a batch_processor.py (Fase 3). "
    "Este m√≥dulo se mantiene como wrapper para compatibilidad."
)
